# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_nyKIhcHYrnqOVuo2Le5xJ6NUq1Y0ibC

1) libraries
"""

#import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import json
import datetime

"""2) Loading raw data"""

from google.colab import drive
drive.mount('/content/drive/')

c_data = pd.read_json('/content/drive/MyDrive/CLV/raw_customer_data - Copy.json', lines=True)

c_data.head()

"""3) Prepocessing the data"""

# Check for missing values
if c_data.isnull().any().any():
    print("Input data contains missing values")
else:
    print("Input data does not contain missing values")

# Select the "transactions" value of the first row and print it
print(c_data.loc[0, 'transactions'])

# Using c_data the original DataFrame converting the "transactions" column to a DataFrame
new_columns = ["id", "settledAt", "ftd", "amount", "status", "type"]
new_data = []

for index, row in c_data.iterrows():
    # extract information from the "transactions" column
    transactions = row["transactions"]
    for transaction in transactions:
        new_row = [row["id"], transaction["settledAt"], transaction["ftd"], 
                   transaction["amount"], transaction["status"], transaction["type"]]
        new_data.append(new_row)

# create the new DataFrame
trans_df = pd.DataFrame(new_data, columns=new_columns)

trans_df.head()

# Extract the transaction date and ftd flag from each transaction dictionary
trans_df['settledAt'] = pd.to_datetime(trans_df['settledAt'])
trans_df['ftd_flag'] = trans_df['ftd'].astype(bool)

# Sort the transactions DataFrame by id and transaction date
trans_df = trans_df.sort_values(['id', 'settledAt'])

# Group the transactions DataFrame by id and filter out all transactions that occurred before the first-time monetary transaction for each customer
trans_df['cum_ftd_flag'] = trans_df.groupby('id')['ftd_flag'].cumsum()
trans_df = trans_df[trans_df['cum_ftd_flag'] > 0]

# Show first 5 rows of DataFrame
trans_df.head()

#Checking for missing values
if trans_df.isnull().any().any():
    print("Transactions data contains missing values")
else:
    print("Transactions data does not contain missing values")

#Checking for duplicated transactions
subset_columns = ["id", "settledAt", "ftd", "amount", "status"]
if trans_df.duplicated(subset=subset_columns).any():
    print("Transactions data contains duplicated transactions")
else:
    print("Transactions data does not contain duplicated transactions")

#Dropping the duplicates
subset_columns = ["id", "settledAt", "ftd", "amount", "status"]
trans_df.drop_duplicates(subset=subset_columns, inplace=True)

#Checking again for duplicated transactions
subset_columns = ["id", "settledAt", "ftd", "amount", "status"]
if trans_df.duplicated(subset=subset_columns).any():
    print("Transactions data contains duplicated transactions")
else:
    print("Transactions data does not contain duplicated transactions")

#Removing outliers of Amount
# number of rows to drop
n = 2 

# get the indices of the n largest values in the "amount" column
max_amount_indices = trans_df["amount"].nlargest(n).index

# drop the rows with the largest values
trans_df.drop(max_amount_indices, inplace=True)

# Verify that the maximum transaction amount has been dropped
max_amount = trans_df['amount'].max()
print('The new maximum transaction amount:', max_amount)

#confirm the data types of the columns are of the appropriate data types
trans_df['settledAt'] = pd.to_datetime(trans_df['settledAt'])
#trans_df['trans_date'] = pd.to_datetime(trans_df['settledAt']).dt.date
trans_df['ftd'] = trans_df['ftd'].astype(bool)
trans_df['amount'] = trans_df['amount'].astype(int)

#Calculating the CLV for each customer
clv_df = trans_df.groupby('id').agg({'amount': 'sum'})
clv_df

#Merge the data
merged_df = pd.merge(trans_df, clv_df, on='id')
merged_df.head()

#Rename amount_y to CLV for each id
merged_df.rename(columns={"amount_y": "CLV"}, inplace=True)
merged_df.head()

#Coverting to time series
# Set the 'settledAt' column as the index of the DataFrame
df = merged_df.set_index('settledAt')
df.head()

# Print the resulting time-series data
print(df)
# Summary statistics
summary_stats = df.describe()
print(summary_stats)

# Plot the histogram of amount
df.hist(column='CLV', bins=100)
plt.title('Histogram of Amount')
plt.xlabel('CLV')
plt.ylabel('Frequency')
plt.show()

# Customizing New the colors of the plot
boxprops = dict(linestyle='-', linewidth=2, color='blue')
whiskerprops = dict(linestyle='--', linewidth=1.5, color='gray')
flierprops = dict(marker='o', markerfacecolor='green', markersize=8, alpha=0.5)
meanprops = dict(marker='D', markeredgecolor='black', markerfacecolor='yellow', markersize=8)
medianprops = dict(linestyle='-', linewidth=2.5, color='green')

# Creating the new box plot
plt.boxplot(df['CLV'], boxprops=boxprops, whiskerprops=whiskerprops, flierprops=flierprops,
            meanprops=meanprops, medianprops=medianprops)

# Adding labels and a title to the new plot
#plt.xlabel('Transaction Amount')
plt.ylabel('CLV')
plt.title('Distribution of Customer CLV')

# Adding grid lines to the plot
plt.grid(axis='y')

# Displaying the plot
plt.show()

"""Model"""

#Required libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import pickle

"""split_data"""

# Split the data into training and testing sets
X = df.drop(['CLV'], axis=1) # features
y = df['CLV'] # target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# One-hot encoding categorical variables
X_train = pd.get_dummies(X_train)
X_test = pd.get_dummies(X_test)

"""train_CLV"""

# Train the model
#rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model = RandomForestRegressor()
rf_model.fit(X_train, y_train)

# Save the model to a pickle file
with open('clv_model.pkl', 'wb') as f:
    pickle.dump(rf_model, f)

"""pred_CLV"""

# Load the saved model from a pickle file
with open('clv_model.pkl', 'rb') as f:
    rf_model = pickle.load(f)

# Use the loaded model to make predictions on new customer data
new_customer_data = X_test
y_pred = rf_model.predict(new_customer_data)

print(y_pred)

"""results"""

# Evaluate the performance of the model
# Compute R-squared
r2 = r2_score(y_test, y_pred)
print("R-squared: {:.2f}".format(r2))

# Compute RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE: {:.2f}".format(rmse))

# Visualize the predicted and actual values
fig, ax = plt.subplots()
ax.scatter(y_test, y_pred, alpha=0.5)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)
plt.title("Actual vs Predicted CLV")
ax.set_xlabel('Actual CLV')
ax.set_ylabel('Predicted CLV')
plt.show()

import seaborn as sns

# Predict the CLV for the test data
y_pred = rf_model.predict(X_test)

# Create a residual plot
sns.residplot(x=y_pred, y=y_test)